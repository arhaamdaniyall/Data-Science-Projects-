{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4552c7",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Every learner should submit his/her own homework solutions. However, you are allowed to discuss the homework with each other– but everyone must submit his/her own solution; you may not copy someone else’s solution.\n",
    "\n",
    "The homework consists of two parts:\n",
    "\n",
    "    1. Data from our lives\n",
    "    2. Variable selection\n",
    "\n",
    "Follow the prompts in the attached jupyter notebook. \n",
    "\n",
    "**We are using the same data as for the previous homework**. Use the version you created called **df2** where you already cleaned, dropped some of the variables and also created the dummy variables.\n",
    "\n",
    "Add markdown cells to your analysis to include your solutions, comments, answers. Add as many cells as you need, for easy readability comment when possible. Hopefully this homework will help you develop skills, make you understand the flow of an EDA, get you ready for individual work.\n",
    "\n",
    "**Note:** This homework has a bonus question, so the highest mark that can be earned is a 105.\n",
    "\n",
    "Submission: Send in both a ipynb and a pdf file of your work.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0c393",
   "metadata": {},
   "source": [
    "# 1. Data from our lives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3b2cd",
   "metadata": {},
   "source": [
    "### Describe a situation or problem from your job, everyday life, current events, etc., for which a variable selection/feature reduction would be appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef2c7c",
   "metadata": {},
   "source": [
    "*Your Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:-A Real Life situation where a variable selection/feature reduction  would be appropriate is in predicting the **LOAN RISK PREDICTION**. Through this system we can predict whether that particular applicant is safe or not to get a LOAN.Variable selection or feature reduction is crucial in the context of loan risk prediction using a regression model. The goal is to identify and include only the most relevant features that significantly contribute to the prediction of loan risk, while excluding irrelevant or redundant ones. This process offers several advantages:\n",
    "**Improved Accuracy:**\n",
    " By performing variable selection in the context of loan risk prediction, the model focuses on the most relevant predictors, leading to improved accuracy. For example, if Credit History and Cash Flow are identified as key factors, the model emphasizes these influential features, resulting in more accurate risk assessments.\n",
    "\n",
    "**Simple Models Are Easier to Interpret**: Feature reduction ensures that only essential variables are included in the model. This simplicity enhances interpretability, making it easier for stakeholders, such as loan officers, to understand and trust the model's decisions. For instance, a straightforward model highlighting the importance of Credit History and Cash Flow simplifies the interpretation process.\n",
    "\n",
    "**Shorter Training Times:** With a reduced set of features, the model training process becomes more efficient. This is particularly important in the context of loan risk prediction, where quick decision-making is crucial. Shorter training times enable faster deployment of models, ensuring timely assessments of loan applications.\n",
    "\n",
    "**Enhanced Generalization by Reducing Overfitting:** Feature reduction mitigates the risk of overfitting by focusing on the most relevant information. Instead of learning noise in the data, the model generalizes better to new, unseen loan applications. This is vital for making accurate predictions and avoiding the pitfalls of overfitting in the dynamic context of loan risk assessment.\n",
    "\n",
    "**Easier to Implement by Software Developers:**  A streamlined model with a reduced set of features is easier for software developers to implement. It simplifies the integration of the model into the bank's existing software infrastructure, allowing for smoother deployment and utilization in real-world lending scenarios.\n",
    "\n",
    "**Reduced Risk of Data Errors by Model Use:** Feature reduction helps improve the overall quality of the data used for training the model. By excluding irrelevant or noisy variables, the model becomes less susceptible to errors caused by misleading information. This ensures that the predictions are based on more reliable and relevant data.\n",
    "\n",
    "**Variable Redundancy:** Techniques like correlation analysis and feature importance ranking help identify and eliminate redundant variables. In the loan risk prediction example, if two features are highly correlated (e.g., Collateral and Capitalization), one of them might be selected while the redundant one is excluded, reducing redundancy and enhancing the model's efficiency.\n",
    "\n",
    "**Avoiding Bad Learning Behavior in High Dimensional Spaces:** High-dimensional spaces can lead to bad learning behavior and increased computational complexity. Feature reduction ensures that the model operates in a more manageable space, reducing the risk of issues associated with high dimensionality and improving the model's overall performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efca705",
   "metadata": {},
   "source": [
    "# 2. Variable selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df663623",
   "metadata": {},
   "source": [
    "In our class so far we covered three types of feature selection techniques. They were: \n",
    "1. Filter methods\n",
    "2. Wrapper methods\n",
    "3. Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc407fc",
   "metadata": {},
   "source": [
    "Use the dataset 'auto_imports1.csv' from our previous homework. More specifically, use the version you created called **df2** where you already cleaned, dropped some of the variables and also created the dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6f8f4",
   "metadata": {},
   "source": [
    "### 2.1. Filtered methods\n",
    "\n",
    "Choose one (you may do more, one is required) of the filtered methods to conduct variable selection. Report your findigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbba7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the stored df2 dataframe from HW2 notebook to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>heights</th>\n",
       "      <th>curb_weight</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>comprassion</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel_type_gas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.6</td>\n",
       "      <td>168.8</td>\n",
       "      <td>64.1</td>\n",
       "      <td>48.8</td>\n",
       "      <td>2548</td>\n",
       "      <td>130</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.6</td>\n",
       "      <td>168.8</td>\n",
       "      <td>64.1</td>\n",
       "      <td>48.8</td>\n",
       "      <td>2548</td>\n",
       "      <td>130</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.5</td>\n",
       "      <td>171.2</td>\n",
       "      <td>65.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>2823</td>\n",
       "      <td>152</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.8</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.2</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2337</td>\n",
       "      <td>109</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.4</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.4</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2824</td>\n",
       "      <td>136</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wheel_base  length  width  heights  curb_weight  engine_size  bore  stroke  \\\n",
       "0        88.6   168.8   64.1     48.8         2548          130  3.47    2.68   \n",
       "1        88.6   168.8   64.1     48.8         2548          130  3.47    2.68   \n",
       "2        94.5   171.2   65.5     52.4         2823          152  2.68    3.47   \n",
       "3        99.8   176.6   66.2     54.3         2337          109  3.19    3.40   \n",
       "4        99.4   176.6   66.4     54.3         2824          136  3.19    3.40   \n",
       "\n",
       "   comprassion  horse_power  peak_rpm  city_mpg  highway_mpg  price  \\\n",
       "0          9.0        111.0    5000.0        21           27  13495   \n",
       "1          9.0        111.0    5000.0        21           27  16500   \n",
       "2          9.0        154.0    5000.0        19           26  16500   \n",
       "3         10.0        102.0    5500.0        24           30  13950   \n",
       "4          8.0        115.0    5500.0        18           22  17450   \n",
       "\n",
       "   fuel_type_gas  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter methods** are generally used as a preprocessing step. \n",
    "The selection of features is independent of any machine learning algorithms. \n",
    "Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 **Basic Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>heights</th>\n",
       "      <th>curb_weight</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>comprassion</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.6</td>\n",
       "      <td>168.8</td>\n",
       "      <td>64.1</td>\n",
       "      <td>48.8</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.6</td>\n",
       "      <td>168.8</td>\n",
       "      <td>64.1</td>\n",
       "      <td>48.8</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.5</td>\n",
       "      <td>171.2</td>\n",
       "      <td>65.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.8</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.2</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2337.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.4</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.4</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2824.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wheel_base  length  width  heights  curb_weight  engine_size  comprassion  \\\n",
       "0        88.6   168.8   64.1     48.8       2548.0        130.0          9.0   \n",
       "1        88.6   168.8   64.1     48.8       2548.0        130.0          9.0   \n",
       "2        94.5   171.2   65.5     52.4       2823.0        152.0          9.0   \n",
       "3        99.8   176.6   66.2     54.3       2337.0        109.0         10.0   \n",
       "4        99.4   176.6   66.4     54.3       2824.0        136.0          8.0   \n",
       "\n",
       "   horse_power  peak_rpm  city_mpg  highway_mpg    price  \n",
       "0        111.0    5000.0      21.0         27.0  13495.0  \n",
       "1        111.0    5000.0      21.0         27.0  16500.0  \n",
       "2        154.0    5000.0      19.0         26.0  16500.0  \n",
       "3        102.0    5500.0      24.0         30.0  13950.0  \n",
       "4        115.0    5500.0      18.0         22.0  17450.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing required packages\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd \n",
    "\n",
    "#setting the threshold\n",
    "threshold = 0.1 \n",
    "#determining the dependent variable \n",
    "X = df2.drop(['price'], axis=1) if 'price' in df2.columns else df2\n",
    "\n",
    "selector = VarianceThreshold(threshold)\n",
    "X_filtered = selector.fit_transform(X)\n",
    "features_to_keep = X.columns[selector.get_support()]\n",
    "df2_filtered = pd.DataFrame(X_filtered, columns=features_to_keep)\n",
    "df2_filtered['price'] = df2['price']\n",
    "df2_filtered.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we have done the basic method of filtering where we removed constant and quasiconstant features. using variance threshold where it removes all features which variance doesn’t meet the threshold. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdd874",
   "metadata": {},
   "source": [
    "### 2.2. Wrapper methods\n",
    "\n",
    "Choose one (you may do more, one is required) of the wrapper methods to conduct variable selection. Report your findigs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **wrapper methods**, the approach involves iteratively selecting subsets of features and training a model with each subset. The decision to include or exclude features from the subset is based on insights gained from the performance of the previous model. Essentially, this process transforms into a search problem, where the goal is to find the most informative combination of features for optimal model performance. However, it's important to note that wrapper methods tend to be computationally intensive due to the exhaustive search over feature subsets, making them resource-demanding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 **Forward Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8484623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Selected Features:\n",
      "['engine_size', 'width', 'horse_power', 'stroke', 'fuel_type_gas', 'peak_rpm']\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "import statsmodels.api as sm\n",
    "#defining the dependent variable\n",
    "target_variable = 'price'\n",
    "# Separating features from target variable\n",
    "X = df2.drop([target_variable], axis=1)\n",
    "y = df2[target_variable]\n",
    "\n",
    "# Setting the significance threshold\n",
    "significance_threshold = 0.01\n",
    "#storing the selected features \n",
    "selected_features = []\n",
    "# Creating the null model\n",
    "null_model = sm.OLS(y, sm.add_constant(pd.Series([1]*len(y), index=y.index))).fit()\n",
    "while True:\n",
    "    best_feature = None\n",
    "    best_pvalue = float('inf')  \n",
    "    \n",
    "    # Iterate through  features \n",
    "    for feature in X.columns:\n",
    "        if feature in selected_features:\n",
    "            continue\n",
    "        \n",
    "        # Adding the features \n",
    "        current_features = selected_features + [feature]\n",
    "        \n",
    "        # Training the model with the current feature set\n",
    "        model = sm.OLS(y, sm.add_constant(X[current_features])).fit()\n",
    "        \n",
    "        # p-value for the added feature\n",
    "        pvalue = model.pvalues[feature]\n",
    "        \n",
    "        # Update the best feature if the one has a smaller p-value\n",
    "        if pvalue < best_pvalue:\n",
    "            best_feature = feature\n",
    "            best_pvalue = pvalue\n",
    "    \n",
    "    # If the best feature's p-value is below the significance threshold, add it to the selected set\n",
    "    if best_pvalue < significance_threshold:\n",
    "        selected_features.append(best_feature)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#final selected features\n",
    "print(\"Final Selected Features:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Stepwise Selection**: It is a method for variable selection in statistical modeling. The process initiates with a Null Model. Subsequently, the method systematically incorporates the most statistically significant variables into the model, one at a time. This iterative addition of variables continues until a predetermined stopping rule is met, or until all the variables under consideration have been included in the model. The primary objective is to refine the model by iteratively introducing the most relevant variables based on statistical significance, ultimately enhancing its predictive capabilities.\n",
    "we determined the threshold value as 0.01. The most significant variable was choosen on the criteria that it has the smallest p-value compared to the given set threshold value.Therefore, the features we got  based on this forward selection process are 'engine_size', 'width', 'horse_power', 'stroke', 'fuel_type_gas', and 'peak_rpm'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501663f3",
   "metadata": {},
   "source": [
    "### 2.3. Embedded methods\n",
    "\n",
    "Choose one (you may do more, one is required) of the embedded methods to conduct variable selection. Report your findigs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 **LASSO Regrresion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c706ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['curb_weight', 'engine_size', 'comprassion', 'horse_power', 'peak_rpm'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Assuming 'target' is your target variable\n",
    "target_variable = 'price'\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df2.drop([target_variable], axis=1)\n",
    "y = df2[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "\n",
    "# Create a LASSO regression model\n",
    "lasso_model = Lasso(alpha=1000)\n",
    "\n",
    "# Fit the LASSO model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features (non-zero coefficients)\n",
    "selected_features = X.columns[lasso_model.coef_ != 0]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO REGRESSION**: It stands for **L**east **A**bsolute **S**hrinkage and **S**election **O**perator,Lasso regression, a variant of linear regression, incorporates a technique known as shrinkage, wherein data values are pulled towards a central point, often the mean. This method is specifically designed to promote the development of uncomplicated, sparse models—models with fewer parameters. Lasso regression is particularly advantageous in scenarios characterized by high levels of multicollinearity or when there's a need to automate aspects of model selection, such as variable selection or parameter elimination.\n",
    "\n",
    "The key mechanism in lasso regression involves L1 regularization, which imposes a penalty equivalent to the absolute value of the coefficients' magnitudes. Regularization, in general, entails introducing a penalty to the various parameters of a machine learning model to constrain its flexibility and, consequently, mitigate the risk of overfitting. In the context of linear models, this penalty is applied to the coefficients that scale each predictor.\n",
    "\n",
    "Lasso's distinctive attribute within the spectrum of regularization techniques is its ability to shrink certain coefficients all the way to zero. Consequently, features associated with these zeroed-out coefficients can be effectively removed from the model. This property makes lasso regression a powerful tool for feature selection, providing a means to streamline models by automatically identifying and excluding less influential predictors.\n",
    "\n",
    " the features that were selected by the LASSO regression model based on their non-zero coefficients are : 'curb_weight', 'engine_size', 'compression', 'horse_power', and 'peak_rpm'.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a783fec",
   "metadata": {},
   "source": [
    "### 2.4. Compare your results\n",
    "Compare your results from the three methods and also compare the coefficients to the full linear regression model (model1) from the previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.860\n",
      "Model:                            OLS   Adj. R-squared:                  0.849\n",
      "Method:                 Least Squares   F-statistic:                     78.89\n",
      "Date:                Sun, 19 Nov 2023   Prob (F-statistic):           5.84e-69\n",
      "Time:                        14:30:22   Log-Likelihood:                -1838.5\n",
      "No. Observations:                 195   AIC:                             3707.\n",
      "Df Residuals:                     180   BIC:                             3756.\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const          -4.45e+04   1.84e+04     -2.419      0.017   -8.08e+04   -8194.301\n",
      "wheel_base       39.5305    103.549      0.382      0.703    -164.796     243.857\n",
      "length          -60.6333     58.500     -1.036      0.301    -176.068      54.801\n",
      "width           603.6414    254.539      2.372      0.019     101.377    1105.906\n",
      "heights         329.5669    140.947      2.338      0.020      51.446     607.688\n",
      "curb_weight       1.1798      1.738      0.679      0.498      -2.249       4.609\n",
      "engine_size     138.4537     16.111      8.594      0.000     106.662     170.245\n",
      "bore          -1208.4137   1206.683     -1.001      0.318   -3589.479    1172.651\n",
      "stroke        -3706.0531    874.513     -4.238      0.000   -5431.669   -1980.437\n",
      "comprassion    -617.1497    446.452     -1.382      0.169   -1498.103     263.804\n",
      "horse_power      34.6328     18.049      1.919      0.057      -0.982      70.248\n",
      "peak_rpm          2.5517      0.709      3.599      0.000       1.153       3.951\n",
      "city_mpg       -288.2868    180.791     -1.595      0.113    -645.030      68.456\n",
      "highway_mpg     316.6334    163.540      1.936      0.054      -6.069     639.336\n",
      "fuel_type_gas -1.173e+04   6002.268     -1.955      0.052   -2.36e+04     110.854\n",
      "==============================================================================\n",
      "Omnibus:                       18.136   Durbin-Watson:                   0.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.211\n",
      "Skew:                           0.240   Prob(JB):                     1.03e-12\n",
      "Kurtosis:                       5.562   Cond. No.                     4.77e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.77e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#loading model1 from HW2 \n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "#opening the model\n",
    "with open('model1.pkl', 'rb') as file:\n",
    "    model1_loaded = pickle.load(file)\n",
    " #printing the summary\n",
    "print(model1_loaded.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2fc672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features Selected by All Methods:\n",
      "{'curb_weight', 'peak_rpm', 'comprassion', 'horse_power', 'engine_size'}\n",
      "\n",
      "Features with Non-Zero Coefficients in Model1:\n",
      "Index(['wheel_base', 'length', 'width', 'heights', 'curb_weight',\n",
      "       'engine_size', 'bore', 'stroke', 'comprassion', 'horse_power',\n",
      "       'peak_rpm', 'city_mpg', 'highway_mpg', 'fuel_type_gas'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assuming model1 is already trained and defined\n",
    "X = df2.drop(columns=[\"price\"])\n",
    "y = df2[\"price\"]\n",
    "X = sm.add_constant(X)\n",
    "model1 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Extract selected features from each method\n",
    "features_method1 = df2_filtered.columns[:-1]  # Exclude the dependent variable 'price'\n",
    "features_method2 = selected_features\n",
    "features_method3 = selected_features\n",
    "\n",
    "# Extract features with non-zero coefficients from the full linear regression model\n",
    "features_model1 = model1.params.index[1:]  # Exclude the intercept\n",
    "\n",
    "# Compare selected features\n",
    "common_features = set(features_method1) & set(features_method2) & set(features_method3) & set(features_model1)\n",
    "\n",
    "print(\"Common Features Selected by All Methods:\")\n",
    "print(common_features)\n",
    "\n",
    "# Compare with features with non-zero coefficients in the full linear regression model\n",
    "print(\"\\nFeatures with Non-Zero Coefficients in Model1:\")\n",
    "print(features_model1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results of feature selection and comparing the selected features:\n",
    "\n",
    "**Common Features Selected by All Methods:**\n",
    "\n",
    "'curb_weight': The weight of the car when it's ready to drive.\n",
    "'peak_rpm': The maximum revolutions per minute of the engine.\n",
    "'comprassion': Compression ratio of the engine.\n",
    "'horse_power': The horsepower of the engine.\n",
    "'engine_size': The size of the car's engine.\n",
    "These features are consistently identified as important across different feature selection methods, suggesting they have a significant impact on predicting the dependent variable 'price.'\n",
    "\n",
    "**Features with Non-Zero Coefficients in Model1:**\n",
    "\n",
    "'wheel_base': The distance between the centers of the front and rear wheels.\n",
    "'length': The length of the car.\n",
    "'width': The width of the car.\n",
    "'height': The height of the car.\n",
    "'curb_weight': The weight of the car when it's ready to drive.\n",
    "'engine_size': The size of the car's engine.\n",
    "'bore': The diameter of the engine cylinders.\n",
    "'stroke': The length of the engine's pistons moving up and down.\n",
    "'compression': Compression ratio of the engine.\n",
    "'horse_power': The horsepower of the engine.\n",
    "'peak_rpm': The maximum revolutions per minute of the engine.\n",
    "'city_mpg': Miles per gallon in the city.\n",
    "'highway_mpg': Miles per gallon on the highway.\n",
    "'fuel_type_gas': Binary indicator for gas fuel type.\n",
    "These features with non-zero coefficients in the full linear regression model (model1) are considered significant in predicting 'price' based on their contribution to the linear regression equation.\n",
    "\n",
    "\n",
    "The common features selected by all methods and the features with non-zero coefficients in model1 represent aspects of the car that strongly influence its price.\n",
    "'engine_size', 'curb_weight', 'horse_power', and 'peak_rpm' appear to be consistently important across all methods, indicating the significant role of the car's engine characteristics.\n",
    "Other features like 'wheel_base', 'length', 'width', 'height', 'bore', 'stroke', 'compression', 'city_mpg', 'highway_mpg', and 'fuel_type_gas' also contribute to predicting the 'price' in the linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be87255",
   "metadata": {},
   "source": [
    "### 2.5 Bonus question (*extra 5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03d112",
   "metadata": {},
   "source": [
    "Reduce your features with PCA. Run a regression with the chosen number of PCA's, report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cff055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Reduced Features:\n",
      "        PC1       PC2       PC3       PC4       PC5    price\n",
      "0 -0.727050 -1.869607 -0.315350  2.645986  0.562464  13495.0\n",
      "1 -0.727050 -1.869607 -0.315350  2.645986  0.562464  16500.0\n",
      "2  0.271794 -1.194953 -1.574672 -0.688328  0.040793  16500.0\n",
      "3 -0.236626 -0.389173 -0.015361 -1.151228  0.318649  13950.0\n",
      "4  1.122627 -1.234495 -0.203774 -1.105527  0.337484  17450.0\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#determining the features column\n",
    "features = ['wheel_base', 'length', 'width', 'heights', 'curb_weight', 'engine_size', 'bore', 'stroke',\n",
    "            'comprassion', 'horse_power', 'peak_rpm', 'city_mpg', 'highway_mpg', 'fuel_type_gas']\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df2[features]\n",
    "y = df2['price']\n",
    "\n",
    "# Standardizing  the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Applying  PCA with 5 components\n",
    "num_components = 5\n",
    "pca = PCA(n_components=num_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Creating a DataFrame with the reduced features\n",
    "columns_pca = [f'PC{i+1}' for i in range(num_components)]\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=columns_pca)\n",
    "\n",
    "# Concatenate the reduced features DataFrame with the target variable\n",
    "df_final = pd.concat([df_pca, y], axis=1)\n",
    "\n",
    "# Display the reduced features DataFrame\n",
    "print(\"DataFrame with Reduced Features:\")\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above Dataframe  with reduced features is obtained through Principal Component Analysis (PCA). Each row represents an observation from our original dataset. The columns 'PC1' through 'PC5' are the principal components resulting from the PCA transformation, and the 'price' is the dependent variable.\n",
    "\n",
    "Each principal component (PC) is a linear combination of the original features.\n",
    "These principal components are orthogonal to each other, and they capture the maximum variance in the data.\n",
    "The values in each row of the 'PC1' to 'PC5' columns represent the coordinates of the data points in the reduced feature space.\n",
    "\n",
    "PC1, PC2, and PC3 have values that vary across different observations, indicating variations in the dataset along these directions.\n",
    "PC4 and PC5, being orthogonal, capture additional variations in the data not covered by the first three components.\n",
    "Price Column:\n",
    "\n",
    "The 'price' column represents the  dependent variable.\n",
    "Each row corresponds to the price of the item associated with the feature values represented by the principal components.\n",
    "Overall Interpretation:\n",
    "\n",
    "The reduced feature space represented by 'PC1' to 'PC5' condenses the information from the original features while retaining the most significant variations.\n",
    "The 'price' column allows you to associate the reduced feature values with the original target variable, facilitating analysis or modeling with a lower-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributions:\n",
      "     wheel_base    length     width   heights  curb_weight  engine_size  \\\n",
      "PC1    0.311628  0.351393  0.342885  0.128115     0.370784     0.330728   \n",
      "PC2    0.196130  0.096381  0.089492  0.263685     0.042755    -0.060528   \n",
      "PC3    0.203115  0.150999 -0.028725  0.574718    -0.059040    -0.263229   \n",
      "PC4   -0.256601 -0.139484 -0.099469 -0.395476     0.019719     0.160582   \n",
      "PC5   -0.087765  0.016078  0.078358 -0.020250     0.059017    -0.040486   \n",
      "\n",
      "         bore    stroke  comprassion  horse_power  peak_rpm  city_mpg  \\\n",
      "PC1  0.275624  0.059112     0.024492     0.303223 -0.096411 -0.321601   \n",
      "PC2 -0.026328  0.132418     0.523976    -0.243137 -0.358649  0.259522   \n",
      "PC3  0.117597 -0.616858    -0.187490    -0.237090 -0.066608 -0.013667   \n",
      "PC4  0.424716 -0.547628     0.128795     0.119776 -0.443762  0.025908   \n",
      "PC5 -0.130874 -0.433406     0.391050     0.191786  0.685783 -0.048707   \n",
      "\n",
      "     highway_mpg  fuel_type_gas  \n",
      "PC1    -0.333626      -0.050302  \n",
      "PC2     0.217083      -0.523654  \n",
      "PC3    -0.026129       0.202554  \n",
      "PC4     0.032601      -0.111159  \n",
      "PC5    -0.049825      -0.332479  \n"
     ]
    }
   ],
   "source": [
    "# Accessing the contribution of each original feature on each principal component\n",
    "contributions = pd.DataFrame(pca.components_, columns=features, index=columns_pca)\n",
    "\n",
    "# Displaying the contibutions \n",
    "print(\"Contributions:\")\n",
    "print(contributions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get to know which original feature contributes how much to each Principle Component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGRESSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PC1      6\n",
       "PC2      6\n",
       "PC3      6\n",
       "PC4      6\n",
       "PC5      6\n",
       "price    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the null value rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PC1      0\n",
       "PC2      0\n",
       "PC3      0\n",
       "PC4      0\n",
       "PC5      0\n",
       "price    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.236\n",
      "Model:                            OLS   Adj. R-squared:                  0.215\n",
      "Method:                 Least Squares   F-statistic:                     11.32\n",
      "Date:                Sun, 19 Nov 2023   Prob (F-statistic):           1.60e-09\n",
      "Time:                        15:41:33   Log-Likelihood:                -1942.6\n",
      "No. Observations:                 189   AIC:                             3897.\n",
      "Df Residuals:                     183   BIC:                             3917.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.304e+04    520.536     25.061      0.000     1.2e+04    1.41e+04\n",
      "PC1         1445.4747    196.829      7.344      0.000    1057.129    1833.821\n",
      "PC2         -308.1532    295.993     -1.041      0.299    -892.152     275.845\n",
      "PC3          -30.1760    457.125     -0.066      0.947    -932.090     871.738\n",
      "PC4         -166.9208    536.047     -0.311      0.756   -1224.548     890.707\n",
      "PC5          853.0238    650.294      1.312      0.191    -430.015    2136.062\n",
      "==============================================================================\n",
      "Omnibus:                      104.481   Durbin-Watson:                   0.471\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              492.529\n",
      "Skew:                           2.186   Prob(JB):                    1.12e-107\n",
      "Kurtosis:                       9.590   Cond. No.                         3.30\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Dropping  rows with NaN values in price \n",
    "df_final = df_final.dropna(subset=['price'])\n",
    "\n",
    "# Extracting  features and price\n",
    "X_pca = df_final[['PC1', 'PC2', 'PC3', 'PC4', 'PC5']]\n",
    "y = df_final['price']\n",
    "\n",
    "# Adding a constant to the features \n",
    "X_pca = sm.add_constant(X_pca)\n",
    "\n",
    "# Creating  the linear regression model\n",
    "model_pca = sm.OLS(y, X_pca).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model_pca.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the statistical significance of each variable is determined by the p-values (P>|t|) associated with their coefficients. A common threshold for statistical significance is a p-value less than 0.05. examining the p-values for each variable:\n",
    "\n",
    "const (Constant): The p-value is 0.000, which is less than 0.05. Therefore, the constant term is statistically significant.\n",
    "\n",
    "PC1: The p-value is 0.000, indicating that PC1 is statistically significant.\n",
    "\n",
    "PC2: The p-value is 0.299, which is greater than 0.05. Therefore, PC2 is not statistically significant at the 0.05 level.\n",
    "\n",
    "PC3: The p-value is 0.947, which is much greater than 0.05. PC3 is not statistically significant.\n",
    "\n",
    "PC4: The p-value is 0.756, which is greater than 0.05. PC4 is not statistically significant.\n",
    "\n",
    "PC5: The p-value is 0.191, which is greater than 0.05. Therefore, PC5 is not statistically significant at the 0.05 level.\n",
    "\n",
    "In summary, the statistically significant variables are the constant term and PC1. These variables have p-values less than 0.05, suggesting that they have a statistically significant impact on the dependent variable 'price' in this regression model. The other variables (PC2, PC3, PC4, and PC5) are not statistically significant based on the 0.05 significance level.\n",
    "\n",
    "the R-squared value of 0.236 indicates that the model explains about 23.6% of the variability in the dependent variable. The low p-value associated with the F-statistic (1.60e-09) suggests that the overall regression model is statistically significant, meaning that there is evidence that at least one of the independent variables is related to the dependent variable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
